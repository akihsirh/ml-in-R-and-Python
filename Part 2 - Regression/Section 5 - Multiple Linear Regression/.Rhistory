sample_size <- 100000
counter <- 0
for(i in rnorm(sample_size)){
if(i <= 1 & i >= -1 ){
counter <- counter+1
}
}
expected_value <- counter/sample_size
expected_value
sample_size <- 100000
counter <- 0
for(i in rnorm(sample_size)){
if(i <= 1 & i >= -1 ){
counter <- counter+1
}
}
expected_value <- counter/sample_size
expected_value
sample_size <- 100000
counter <- 0
for(i in rnorm(sample_size)){
if(i <= 1 & i >= -1 ){
counter <- counter+1
}
}
expected_value <- counter/sample_size
expected_value
sample_size <- 100000
counter <- 0
for(i in rnorm(sample_size)){
if(i <= 1 & i >= -1 ){
counter <- counter+1
}
}
expected_value <- counter/sample_size
expected_value
sample_size <- 100000
for(loop in 1:5){
counter <- 0
for(i in rnorm(sample_size)){
if(i <= 1 & i >= -1 ){
counter <- counter+1
}
}
expected_value <- counter/sample_size
print
sample_size <- 100000
for(loop in 1:5){
counter <- 0
for(i in rnorm(sample_size)){
if(i <= 1 & i >= -1 ){
counter <- counter+1
}
}
expected_value <- counter/sample_size
print(expected_value)
}
MyFirstVector <- c(3,45,56,732)
MyFirstVector
is.numeric(MyFirstVector)
is.integer(MyFirstVector)
is.double(MyFirstVector)
MyFirstVector <- c(3l,45l,732l)
MyFirstVector <- c(3L,45L,732L)
MyFirstVector
is.numeric(MyFirstVector)
is.integer(MyFirstVector)
is.double(MyFirstVector)
CVec <- c("A","b","123")
CVec
is.numeric(CVec)
is.character(CVec)
S1Vec <- c("a","g",7)
S1Vec
is.numeric(S1Vec)
is.character(S1Vec)
seq(1,15)
1:15
z<- seq(1,15,4)
z
rep1 <- rep(6,21)
rep1
repc <- rep("A",35)
repc
repv <- rep(c(3,4),10)
rep1 <- rep(6,21)repv
repv
z1 <- seq(20,35,6)
z1
z2 <- seq(20:35,40)
z2 <- seq(20:21)
z2
z2
z2 <- seq(20:21)
z2
z2 <- seq(20:22)
z2
z2 <- seq(20:23)
z2
z2 <- seq(20:30)
z2
MyFirstVector[1]
MyFirstVector[3]
z[-9]
z<- seq(1,15,4)
z
z<- seq(1,15)
z
z[-9]
z[-20]
z[20]
MyFirstVector[-1:-3]
MyFirstVector[1:3]
z[4:7]
z[c(8,11,12)]
z[c(-2,-5)]
z[-3:-6]
z[-1:1]
z[seq(2:5)]
z[seq(2,5)]
z[rep(3,5)]
z[0]
z[102]
z[-102]
zero <- z[0]
zero
sv1 <- seq(1:15)
sv2 <- seq(16:20)
svadd <- sv1 + sv2
svadd
svsub <- sv2 - sv1
svsub
#Vector Arithmatics
sv1 <- seq(1,15)
sv2 <- seq(16,20)
#Vector Addition
svadd <- sv1 + sv2
svadd
svsub <- sv2 - sv1
svsub
svsub1 <- sv1 - sv2
svsub1
svmul <- sv1 * sv2
svmul
svdiv <- sv1 /sv2
svdiv
svmod <- sv2 % sv1
svmod
sv3 <- seq(1,30)
sv4 <- sv2 + sv3
sv4
sv5 <- seq(1,19)
sv6 <- sv5 + sv3
sv5 <- seq(1,19)
sv6 <- sv5 + sv3
sv6
x <- rnorm(5)
x
x <- rnorm(5)
x
for(i in x){
print(i)
}
# programming generic loop
for(j in 1:5){
print(j)
}
# R specific programming loop
for(i in x){
print(i)
}
# programming generic loop
for(j in 1:5){
print(x[j])
}
N <- 100
r <- rnorm(N)
s <- rnorm(N)
# ------------------------------- vectorized ways -----------------
N <- 100
r <- rnorm(N)
s <- rnorm(N)
#devectorized multiplication
t <- rep(NA,N)
for(i in 1:N){
t[i] <- r[i] * s[i]
}
#vectorized multiplication
u <- r * s
?seq
z<- seq(1,15,4)
z
z1 <- seq(20,35,6)
z1
z2 <- seq(20:30)
z2
z3 <- seq(from=35, to = 55, length.out = 15)
z3
seqlen <- c("q", "w","e","r","t","y")
z4 <- seq(from=35, to = 55, along.with = seqlen)
z4
repseq1 <- rep(5:6,10)
repseq1
repseq1 <- rep(5:6,times = 10)
repseq1
repseq2 <- rep(5:6,each = 10)
repseq2
b <- sqrt(repseq1)
b
setwd("C:/Git/ML-R-and-Python/Part 2 - Regression/Section 5 - Multiple Linear Regression")
# Data Preprocessing Template
# Importing the dataset
dataset = read.csv('50_Startups.csv')
View(dataset)
View(dataset)
#Encode the categorical data state
dataset$State <- factor(x = dataset$State,
levels = c("New York","California","Florida"),
labels = c(1,2,3))
# Splitting the dataset into the Training set and Test set
# install.packages('caTools')
library(caTools)
set.seed(123)
split = sample.split(dataset$Profit, SplitRatio = 0.8)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
# Feature Scaling wont be applied since the linear regression libraries support feature scaling
#Fit the multiple linear regression model to the training set
regressor <- lm(formula = Profit ~ . , #efficient way to say Profit depends on all variables
data = training_set)
summary(regressor)
?lm
#predicting the test set with our trained regressor
y_predicted <- predict(regressor, newdata = test_set)
summary(regressor)
coef(summary(regressor))
length(training_set)
ncol(training_set)
nrow(training_set)
typeof(coef(summary(regressor)))
is.data.frame(coef(summary(regressor)))
justForMe <- coef(summary(regressor))
is.matrix(coef(summary(regressor)))
# Data Preprocessing Template
# Importing the dataset
dataset = read.csv('50_Startups.csv')
#Encode the categorical data state
dataset$State <- factor(x = dataset$State,
levels = c("New York","California","Florida"),
labels = c(1,2,3))
# Splitting the dataset into the Training set and Test set
# install.packages('caTools')
library(caTools)
set.seed(123)
split = sample.split(dataset$Profit, SplitRatio = 0.8)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
# Intercepts are considered by the lm function unlike Python where we had to add an extra column
# Feature Scaling wont be applied since the linear regression libraries support feature scaling
# R also created the dummy variable for the encoded variable and like Python automatically
#handles the dummy variable trap
#Fit the multiple linear regression model to the training set
backwardElimination <- function(x, siglevel) {
numPredictors = length(x) # equivalent to ncol
for (i in c(1:numPredictors)){
regressor = lm(formula = Profit ~ ., data = x)
maxVar = max(coef(summary(regressor))[c(2:numPredictors), "Pr(>|t|)"]) #matrix of stats
if (maxVar > siglevel){
j = which(coef(summary(regressor))[c(2:numVars), "Pr(>|t|)"] == maxVar) #Location or variable number
x = x[, -j] # remove max p value predictor
}
numVars = numVars - 1
}
return(summary(regressor))
}
SL = 0.05
training_set_opt = training_set[, c(1,2,3,4,5)]
backwardElimination(training_set_opt, SL)
#predicting the test set with our trained regressor
y_predicted <- predict(regressor, newdata = test_set)
#Plot the data to better understand the regressor accuracy
# Data Preprocessing Template
# Importing the dataset
dataset = read.csv('50_Startups.csv')
#Encode the categorical data state
dataset$State <- factor(x = dataset$State,
levels = c("New York","California","Florida"),
labels = c(1,2,3))
# Splitting the dataset into the Training set and Test set
# install.packages('caTools')
library(caTools)
set.seed(123)
split = sample.split(dataset$Profit, SplitRatio = 0.8)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
# Intercepts are considered by the lm function unlike Python where we had to add an extra column
# Feature Scaling wont be applied since the linear regression libraries support feature scaling
# R also created the dummy variable for the encoded variable and like Python automatically
#handles the dummy variable trap
#Fit the multiple linear regression model to the training set
backwardElimination <- function(x, siglevel) {
numPredictors = length(x) # equivalent to ncol
for (i in c(1:numPredictors)){
regressor = lm(formula = Profit ~ ., data = x)
maxVar = max(coef(summary(regressor))[c(2:numPredictors), "Pr(>|t|)"]) #matrix of stats
if (maxVar > siglevel){
j = which(coef(summary(regressor))[c(2:numPredictors), "Pr(>|t|)"] == maxVar) #Location or variable number
x = x[, -j] # remove max p value predictor
}
numVars = numVars - 1
}
return(summary(regressor))
}
SL = 0.05
training_set_opt = training_set[, c(1,2,3,4,5)]
backwardElimination(training_set_opt, SL)
#predicting the test set with our trained regressor
y_predicted <- predict(regressor, newdata = test_set)
#Plot the data to better understand the regressor accuracy
# Data Preprocessing Template
# Importing the dataset
dataset = read.csv('50_Startups.csv')
#Encode the categorical data state
dataset$State <- factor(x = dataset$State,
levels = c("New York","California","Florida"),
labels = c(1,2,3))
# Splitting the dataset into the Training set and Test set
# install.packages('caTools')
library(caTools)
set.seed(123)
split = sample.split(dataset$Profit, SplitRatio = 0.8)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
# Intercepts are considered by the lm function unlike Python where we had to add an extra column
# Feature Scaling wont be applied since the linear regression libraries support feature scaling
# R also created the dummy variable for the encoded variable and like Python automatically
#handles the dummy variable trap
#Fit the multiple linear regression model to the training set
backwardElimination <- function(x, siglevel) {
numPredictors = length(x) # equivalent to ncol
for (i in c(1:numPredictors)){
regressor = lm(formula = Profit ~ ., data = x)
maxVar = max(coef(summary(regressor))[c(2:numPredictors), "Pr(>|t|)"]) #matrix of stats
if (maxVar > siglevel){
j = which(coef(summary(regressor))[c(2:numPredictors), "Pr(>|t|)"] == maxVar) #Location or variable number
x = x[, -j] # remove max p value predictor
}
numPredictors = numPredictors - 1
}
return(summary(regressor))
}
SL = 0.05
training_set_opt = training_set[, c(1,2,3,4,5)]
backwardElimination(training_set_opt, SL)
#predicting the test set with our trained regressor
y_predicted <- predict(regressor, newdata = test_set)
#Plot the data to better understand the regressor accuracy
# Data Preprocessing Template
# Importing the dataset
dataset = read.csv('50_Startups.csv')
#Encode the categorical data state
dataset$State <- factor(x = dataset$State,
levels = c("New York","California","Florida"),
labels = c(1,2,3))
# Splitting the dataset into the Training set and Test set
# install.packages('caTools')
library(caTools)
set.seed(123)
split = sample.split(dataset$Profit, SplitRatio = 0.8)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
# Intercepts are considered by the lm function unlike Python where we had to add an extra column
# Feature Scaling wont be applied since the linear regression libraries support feature scaling
# R also created the dummy variable for the encoded variable and like Python automatically
#handles the dummy variable trap
#Fit the multiple linear regression model to the training set
backwardElimination <- function(x, siglevel) {
numPredictors = length(x) # equivalent to ncol
for (i in c(1:numPredictors)){
regressor = lm(formula = Profit ~ ., data = x)
maxVar = max(coef(summary(regressor))[c(2:numPredictors), "Pr(>|t|)"]) #matrix of stats
if (maxVar > siglevel){
j = which(coef(summary(regressor))[c(2:numPredictors), "Pr(>|t|)"] == maxVar) #Location or variable number
x = x[, -j] # remove max p value predictor
}
numPredictors = numPredictors - 1
}
return(summary(regressor))
}
SL = 0.05
training_set_opt = training_set[, c(1,2,3,4,5)]
regressor <- backwardElimination(training_set_opt, SL)
#predicting the test set with our trained regressor
y_predicted <- predict(regressor, newdata = test_set)
#Plot the data to better understand the regressor accuracy
# Data Preprocessing Template
# Importing the dataset
dataset = read.csv('50_Startups.csv')
#Encode the categorical data state
dataset$State <- factor(x = dataset$State,
levels = c("New York","California","Florida"),
labels = c(1,2,3))
# Splitting the dataset into the Training set and Test set
# install.packages('caTools')
library(caTools)
set.seed(123)
split = sample.split(dataset$Profit, SplitRatio = 0.8)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
# Intercepts are considered by the lm function unlike Python where we had to add an extra column
# Feature Scaling wont be applied since the linear regression libraries support feature scaling
# R also created the dummy variable for the encoded variable and like Python automatically
#handles the dummy variable trap
#Fit the multiple linear regression model to the training set
backwardElimination <- function(x, siglevel) {
numPredictors = length(x) # equivalent to ncol
for (i in c(1:numPredictors)){
regressor = lm(formula = Profit ~ ., data = x)
maxVar = max(coef(summary(regressor))[c(2:numPredictors), "Pr(>|t|)"]) #matrix of stats
if (maxVar > siglevel){
j = which(coef(summary(regressor))[c(2:numPredictors), "Pr(>|t|)"] == maxVar) #Location or variable number
x = x[, -j] # remove max p value predictor
}
numPredictors = numPredictors - 1
}
return(summary(regressor))
}
SL = 0.05
training_set_opt = training_set[, c(1,2,3,4,5)]
regressor = backwardElimination(training_set_opt, SL)
#predicting the test set with our trained regressor
y_predicted <- predict(regressor, newdata = test_set)
#Plot the data to better understand the regressor accuracy
#Fit the multiple linear regression model to the training set
rm(regressor)
# Data Preprocessing Template
# Importing the dataset
dataset = read.csv('50_Startups.csv')
#Encode the categorical data state
dataset$State <- factor(x = dataset$State,
levels = c("New York","California","Florida"),
labels = c(1,2,3))
# Splitting the dataset into the Training set and Test set
# install.packages('caTools')
library(caTools)
set.seed(123)
split = sample.split(dataset$Profit, SplitRatio = 0.8)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
# Intercepts are considered by the lm function unlike Python where we had to add an extra column
# Feature Scaling wont be applied since the linear regression libraries support feature scaling
# R also created the dummy variable for the encoded variable and like Python automatically
#handles the dummy variable trap
#Fit the multiple linear regression model to the training set
backwardElimination <- function(x, siglevel) {
numPredictors = length(x) # equivalent to ncol
for (i in c(1:numPredictors)){
regressor = lm(formula = Profit ~ ., data = x)
maxVar = max(coef(summary(regressor))[c(2:numPredictors), "Pr(>|t|)"]) #matrix of stats
if (maxVar > siglevel){
j = which(coef(summary(regressor))[c(2:numPredictors), "Pr(>|t|)"] == maxVar) #Location or variable number
x = x[, -j] # remove max p value predictor
}
numPredictors = numPredictors - 1
}
return(summary(regressor))
}
SL = 0.05
training_set_opt = training_set[, c(1,2,3,4,5)]
backwardElimination(training_set_opt, SL)
#predicting the test set with our trained regressor
y_predicted <- predict(regressor, newdata = test_set)
#Plot the data to better understand the regressor accuracy
# Data Preprocessing Template
# Importing the dataset
dataset = read.csv('50_Startups.csv')
#Encode the categorical data state
dataset$State <- factor(x = dataset$State,
levels = c("New York","California","Florida"),
labels = c(1,2,3))
# Splitting the dataset into the Training set and Test set
# install.packages('caTools')
library(caTools)
set.seed(123)
split = sample.split(dataset$Profit, SplitRatio = 0.8)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
# Intercepts are considered by the lm function unlike Python where we had to add an extra column
# Feature Scaling wont be applied since the linear regression libraries support feature scaling
# R also created the dummy variable for the encoded variable and like Python automatically
#handles the dummy variable trap
#Fit the multiple linear regression model to the training set
regressor = lm(formula = Profit ~ ., data = training_set)
backwardElimination <- function(x, siglevel) {
numPredictors = length(x) # equivalent to ncol
for (i in c(1:numPredictors)){
regressor = lm(formula = Profit ~ ., data = x)
maxVar = max(coef(summary(regressor))[c(2:numPredictors), "Pr(>|t|)"]) #matrix of stats
if (maxVar > siglevel){
j = which(coef(summary(regressor))[c(2:numPredictors), "Pr(>|t|)"] == maxVar) #Location or variable number
x = x[, -j] # remove max p value predictor
}
numPredictors = numPredictors - 1
}
return(summary(regressor))
}
SL = 0.05
training_set_opt = training_set[, c(1,2,3,4,5)]
backwardElimination(training_set_opt, SL)
#predicting the test set with our trained regressor
y_predicted <- predict(regressor, newdata = test_set)
#Plot the data to better understand the regressor accuracy
